---
layout: post
title:  Il protone "intelligente"
date:   2018-03-01
author: Milano
categories:  
permalink: /:categories/:year/:month/:day/:title/
hidden: true
---

Come il metodo Monte Carlo e l’uso di reti neurali ha rivoluzionato lo studio delle Parton Distribution Functions (PDFs) del protone.

_(A cura di [Giovanni Stagnitto](mailto:giovanni.stagnitto@gmail.com), Università degli Studi di Milano)_

Dopo la meritata euforia “domenicale” seguita alla scoperta del bosone di Higgs, negli ultimi anni il mondo della fisica delle particelle sta vivendo un travagliato “lunedì”: i tanto bramati indizi di nuova fisica oltre il Modello Standard delle interazioni fondamentali tardano ad arrivare. In attesa di aumentare l’energia del Large Hadron Collider (LHC) per esplorare nuovi regimi cinematici e quindi permettere la produzione di eventuali nuove particelle, l’obiettivo è mettere alla prova sempre di più le predizioni del Modello Standard, grazie a misure più accurate o di piccole deviazioni dei dati rispetto alle previsioni teoriche, alla ricerca di ulteriori conferme sperimentali. Precisione e accuratezza sono pertanto le parole chiave che guidano il lavoro di fisici teorici e sperimentali in questo decennio. Per un acceleratore di protoni e ioni come LHC la Quantum Chromodynamics (QCD), quella parte di Modello Standard che descrive le interazioni forti, è di fondamentale importanza.

<ul class="collapsible" data-collapsible="accordion">
    <li>
      <div class="collapsible-header"><i class="material-icons">live_help</i> Il modello a quark </div>
      <div class="collapsible-body">
          <p>
           Il protone non è una particella elementare, come per esempio l’elettrone o il muone. La sua struttura interna è molto complessa, anche se ad energie ordinarie buona parte delle sue proprietà possono essere spiegate assumendo che sia costituito da due quark up (con carica +⅔) e un quark down (con carica -⅓), e questi quark siano legati tra loro dai mediatori della forza forte, i cosiddetti gluoni. La loro presenza è necessaria anche per giustificare la massa del protone, ordini di grandezza maggiore della somma delle masse dei  quark up e down: infatti la massa del protone è data per il 99.9% dall’energia di legame tra i costituenti.
Oltre ai quark up e down, esistono anche altri tipi di quark, di flavour diverso: strange (s), charm (c), bottom (b) e top (t), ognuno con un valore di massa diverso. In particolare gli ultimi due sono molto più pesanti dei precedenti e tale gerarchia nelle masse dei quark è un grande mistero del Modello Standard.
          </p>
      </div>
    </li>
</ul>

All’interno della fisica di LHC, un ruolo particolare è svolto dalle cosiddette distribuzioni di struttura partonica del protone, in inglese Parton Distribution Functions (PDFs), che sono l’oggetto di questo articolo. Per capire come esse entrino in gioco, occorre fare una breve digressione sul concetto che sta alla base delle predizioni di QCD per i processi di LHC, ovvero la cosiddetta _fattorizzazione_, che può essere compresa grazie all’aiuto della Figura 1, in cui è schematizzato lo scattering di due protoni A e B.

<div class="row">
<div class="col s12 m6 offset-m3">
<img src="/sistemidiriferimento/img/18_03_01_PRT/scattering.png"/>
</div>
</div>

{:.image-caption}
*Figura 1: Rappresentazione schematica della fattorizzazione [1].*

L’alta energia di LHC agisce come una “lente di ingrandimento” dotata di grande risoluzione e ci permette di osservare direttamente i costituenti interni del protone, i cosiddetti _partoni_ (quark e gluoni): l’etichetta a (b) in Figura 1 indica proprio questo. Si può dire, quindi, che il processo avviene non direttamente tra i protoni, ma tra i partoni che, estratti dal grembo protonico materno con una certa frazione $x$ di quantità di moto, interagiscono dando luogo a stati finali rivelati dai detector che circondano la linea del fascio. La fattorizzazione consiste nell’affermare che il calcolo del processo possa venire fattorizzato in due contributi:

$$\sigma = [f_{a}(x_{1})  f_{b}(x_{2})] [\hat{\sigma}_{a,b}(x_{1},x_{2})]$$

Il fattore a destra indica il processo di interazione tra partoni e può essere calcolato da principi primi in modo _perturbativo_: si espande la sezione d’urto in una serie di potenze nella costante $\alpha_s$, che rappresenta l’intensità del legame tra partoni e diminuisce sempre più al crescere dell’energia (_libertà asintotica_), e di questa serie si possono considerare solo i primi termini.
Il fattore a sinistra invece codifica tutta la dinamica _non perturbativa_ delle basse energie: l’espansione in serie prima citata non è più possibile. A tutti gli effetti una mistery box, che racchiude in sé la risposta alla domanda “Quale è la probabilità di estrarre dal grembo di mamma protone un quark di un determinato flavour a/b che porti una certa frazione $x_{1} (x_{2})$ di quantità di moto del protone?”. Questo oggetto si chiama Parton Distribution Function (PDF).

Esse sono state a lungo studiate dal punto di vista teorico ed è noto che dipendono da due variabili, ovvero dall’energia Q<sup>2</sup> con cui viene sondato il protone (la quale può essere considerata come detto prima il grado di “risoluzione” della nostra lente) e dalla frazione di quantità di moto $x$ con cui il partone di un generico flavour f viene estratto dal protone. Mentre la dipendenza delle PDFs da $x$ può essere ricavata solo dai dati sperimentali, la dipendenza da Q<sup>2</sup> è descritta dalle equazioni di Alterelli e Parisi (o anche DGLAP), un grande successo della QCD perturbativa (l’articolo di Altarelli e Parisi è uno degli articoli più citati di tutti i tempi) [2]. Tuttavia, per poter risolvere le DGLAP è necessario conoscere le condizioni iniziali, che devono essere anch’esse ricavate dai dati sperimentali (a causa del carattere non perturbativo della dinamica).

Osservando la Figura 2, nella quale è rappresentata la dipendenza delle PDFs da $x$ (notare la scala logaritmica) per due scale di energia diverse (a sinistra una scala “bassa” mentre a destra una scala “alta” tipica di LHC), possiamo fare qualche considerazione qualitativa sulla struttura interna del protone. Per alti valori di $x$ (grandi frazioni di momento) troviamo due picchi corrispondenti ai quark up e al quark down, con circa un rapporto 2:1, mentre gli altri flavour sono praticamente assenti: per questo motivo si dice che il protone sia composto da due quark up e da un quark down. Al diminuire di $x$ (quindi per frazioni di momento via via minori), comincia a crescere anche il contributo di quark anche di flavour diverso da u e d, generati a coppie secondo il processo di splitting $g\rightarrow quark+antiquark$. Leader indiscusso per bassi valori di $x$ è però il gluone (notare il fattore 1/10) e tale caratteristica si accentua ancora di più con il crescere della scala Q<sup>2</sup>.

<div class="row">
<div class="col s12 m6 offset-m3">
<img src="/sistemidiriferimento/img/18_03_01_PRT/pdf.png"/>
</div>
</div>

{:.image-caption}
*Figura 2: Dipendenza delle PDFs da $x$ (notare la scala logaritmica) per due scale di energia diverse (a sinistra una scala “bassa” mentre a destra una scala “alta” tipica di LHC)  [3].*

## Come si determinano le PDFs?

Il metodo standard per determinare le PDFs si basa su una parametrizzazione a priori della loro forma funzionale, stabilita in base a considerazioni teoriche. La predizione teorica viene espressa in termini dei parametri ignoti presenti nelle PDFs e il confronto con il dato sperimentale permette di risalire al loro valore. Una volta ottenuta una stima dell’andamento delle PDFs, è opportuno come in ogni buona predizione fisica quantificare la nostra incertezza, andando a stabilire opportuni intervalli di confidenza. L’assunzione di una particolare parametrizzazione forza il sistema a convergere in una determinata direzione, escludendo di fatto altre possibili forme funzionali. Ciò comporta uno sottostima delle incertezze e a risultati talvolta insensati.

Inoltre, la determinazione dei parametri del modello in genere non è una procedura univoca, ma dipende fortemente dal sottoinsieme (ristretto) di dati sperimentali su cui ci si basa. Per questo quando la predizione è confrontata con altri dati, succede spesso che questi si trovino ben al di fuori dell’intervallo di confidenza stimato (ovvero la probabilità che questo accada è maggiore dei livelli di confidenza associati agli intervalli).

Un metodo alternativo per la determinazione delle PDFs in questo articolo è stato introdotto dalla collaborazione <a href="http://nnpdf.mi.infn.it/">NNPDF</a> (Neural Networks PDF) nello scorso decennio, e prevede l’uso massiccio di tecniche di intelligenza artificiale. La collaborazione, guidata dal prof. Stefano Forte dell’Università degli Studi di Milano, conta attualmente 15 membri di 12 diverse istituzioni in 5 stati europei e fornisce periodicamente alla comunità scientifica set aggiornati di PDFs, includendo gradualmente i nuovi dati sperimentali e migliorando il metodo di analisi (l’ultima versione 3.1 è del giugno 2017). L’importanza di aggiungere sempre più rilevazioni sperimentali sta nel fatto che ogni particolare osservabile permette di esplorare zone diverse del piano Q<sup>2</sup>-$x$ (vedi Figura 3) e soprattutto alcune osservabili sono più sensibili a determinate PDFs rispetto ad altre.

<div class="row">
<div class="col s12 m6 offset-m3">
<img src="/sistemidiriferimento/img/18_03_01_PRT/kinematic.png"/>
</div>
</div>

{:.image-caption}
*Figura 3: Disposizione dei dati sperimentali utilizzati nei fit di PDF all'interno del piano $x$ - Q<sup>2</sup> [3].*


Da questo punto fino al termine dell’articolo, tenteremo di spiegare con parole semplici come funzionano le tecniche di machine learning adottate dalla collaborazione. Partiamo dai dati: assumiamo che ogni dato rappresenti il valore medio di una distribuzione di probabilità e che la deviazione standard sperimentale rappresenti la larghezza di tale distribuzione. Andiamo quindi a costruire un campione di eventi facsimile in modo che i parametri medi di questo campione, all’aumentare della numerosità del campione, tendano ai valori veri (metodi di questo tipo basati sulla generazione casuale di eventi prendono il nome di simulazioni Monte Carlo, in onore del celebre porto francese patria dei casinò).

A questo punto occorre parametrizzare opportunamente le PDFs. Per fare questo, la collaborazione NNPDF si appoggia sull’utilizzo di una rete neurale. In estrema sintesi, la rete neurale è ciò che modellizza la mistery box di cui abbiamo parlato prima, la quale collega a fissato input ($x$) un fissato output (i punti sperimentali sotto forma di repliche alla Monte Carlo).
L’idea è quella di simulare un passaggio di informazioni tra “neuroni” interconnessi mediante “sinapsi” di collegamento. Ogni neurone è caratterizzato da un parametro $\xi_i$, mentre la sinapsi che connette i neuroni i e j è caratterizzata da un peso $\omega_{ij}$. Lo stato $\xi_i$ è una funzione $g$ dei pesi $\omega_{ij}$ delle sinapsi che lo raggiungono, degli stati nei neuroni circostanti $\xi_j$ e di una soglia $\theta_i$ che sta a indicare l’attivazione o meno del neurone:

$$\xi_i = g\left(\sum_{j} \xi_j w_{ij} - \theta_i\right)$$

Un esempio di funzione g di solito presa come modello è rappresentata nella Figura 4.

<div class="row">
<div class="col s12 m6 offset-m3">
<img src="/sistemidiriferimento/img/18_03_01_PRT/g.png"/>
</div>
</div>

{:.image-caption}
*Figura 4:  Tipica funzione di attivazione usata nelle reti neurali: $g(z) = \frac{1}{1+e^{-z}}$.*

Tale funzione è in grado di riprodurre comportamenti non lineari: mentre è caratterizzata da una risposta lineare nei dintorni di $z\sim 0$ e satura per grandi valori negativi e positivi di $z$, con un’opportuna taratura dei pesi e delle soglie, si può fare in modo che essa lavori nelle zone di transizione tra il regime lineare e di saturazione, e che quindi possa riprodurre andamenti anche non triviali. Grazie a questo meccanismo, ogni funzione continua può essere modellizzata mediante una rete neurale.

<div class="row">
<div class="col s12 m6 offset-m3">
<img src="/sistemidiriferimento/img/18_03_01_PRT/hidden.png"/>
</div>
</div>

{:.image-caption}
*Figura 5: Esempio di topologia tipica di rete neurale, organizzata su 3 layer: input - hidden - output [4].*

Una volta compreso il meccanismo di funzionamento, occorre trovare i giusti valori dei parametri $\xi$ e $\omega$ per descrivere la funzione che, partendo dall’input introdotto, restituisca l’output desiderato. Ciò può essere fatto iterativamente. Vengono casualmente scelti parametri iniziali della rete neurale; al termine di ogni iterazione viene calcolato un funzionale che esprime la deviazione tra l’output attuale e l’output richiesto e i parametri della rete neurale vengono modificati a catena in modo che il funzionale diminuisca. Così via, per un ottimale numero di iterazioni, una rete neurale diversa viene modellata per descrivere al meglio ogni diversa replica dei dati sperimentali.

Parliamo di valore "ottimale” perchè il rischio nel tenere accesa troppo a lungo la macchina neurale è quello di un apprendimento eccessivo che giunga a cogliere il rumore statistico presente nell’insieme dei dati. Visivamente si può cogliere questo fatto nella Figura 6, nella quale i punti vuoti con la barra di errore rappresentano i dati sperimentali, mentre i punti pieni sono l'output della rete neurale dopo un diverso numero di iterazioni di apprendimento: poche iterazioni (sotto-apprendimento), il “giusto” numero (apprendimento ideale), eccessive iterazioni (sovra-apprendimento).

<div class="row">
<div class="col s12 m6 offset-m3">
<img src="/sistemidiriferimento/img/18_03_01_PRT/plot.png"/>
</div>
</div>

{:.image-caption}
*Figura 6: Risultato dell'output della rete neurale dopo un numero diverso di iterazioni (vedi testo) [4].*

L’affermazione riguardo al fatto che quello sia il “giusto” numero nasce dall’occhio dell’uomo che “vede” una ragionevole curva liscia seguire l’andamento dei punti sperimentali, ma come può una macchina scegliere quando fermarsi? Un criterio oggettivo è quello del _cross-validation_: i punti sperimentali vengono suddivisi arbitrariamente in due gruppi, un gruppo di apprendimento e uno di validazione, ma solo i punti del gruppo di apprendimento vengono considerati ai fini del _fitting_; ovvero in altre parole si minimizza solo il $\chi^2$ di apprendimento, monitorando però allo stesso tempo anche il $\chi^2$ di validazione. Ebbene, il punto di apprendimento ideale è definito come il numero di iterazioni in corrispondenza del quale il $\chi^2$ di validazione smette di diminuire, a differenza di quello di apprendimento. Nella Figura 7 è mostrato un esempio esplicito della procedura: a sinistra si vede chiaramente come il numero di iterazioni ideale si assesti intorno a 2000.

<div class="row">
<div class="col s12 m6 offset-m3">
<img src="/sistemidiriferimento/img/18_03_01_PRT/chi2.png"/>
</div>
</div>

{:.image-caption}
*Figura 7: Metodo cross-validation per stabilire il corretto numero di iterazioni della rete neurale [5].*

Una volta terminata l’iterazione della rete neurale per ogni replica Monte Carlo dei dati sperimentali, abbiamo a disposizione un set di repliche di PDF  e otteniamo la PDF ricercata come media di questo set. Ma non solo: uno dei grandi vantaggi del metodo NNPDF è quello di fornire allo stesso tempo anche intervalli di incertezza affidabili, considerando ad esempio la deviazione standard del set delle repliche come intervallo $1-\sigma$. Tale intervallo ha anche un’altra importante caratterizzazione: su 100 PDF generate, 68 saranno contenute in tale intervallo, come mostrato nella Figura 8 per la PDF del gluone.

<div class="row">
<div class="col s12 m6 offset-m3">
<img src="/sistemidiriferimento/img/18_03_01_PRT/n100.png"/>
</div>
</div>

{:.image-caption}
*Figura 8: 100 repliche della PDF del gluone (linee verdi), con loro valore centrale e intervallo $\pm 1-\sigma$ attorno al valore centrale (linee rosse) [5].*

Riassumendo, il metodo basato sull’uso di reti neurali ha provocato una piccola rivoluzione in questo campo affascinante e di grande importanza per la fisica delle alte energie dei prossimi anni. Il prof. Forte ha recentemente vinto un Advanced Grant bandito dall’European Research Council, grazie al quale sarà portato avanti lo studio di tecniche di intelligenza artificiale per le PDFs. Il progetto è chiamato <a href="http://n3pdf.mi.infn.it/">N3PDF</a> e sono attualmente aperte due posizioni (con scadenza il 1 aprile) per studenti che cominceranno il dottorato il prossimo anno (<a href="https://academicjobsonline.org/ajo/jobs/10735">https://academicjobsonline.org/ajo/jobs/10735</a>). Se avete trovato interessati gli argomenti trattati in questo articolo, perchè non applicare?

## Per approfondire

[1] J. M. Campbell, J. W. Huston, W. J. Stirling. "<a href="http://arxiv.org/abs/hep-ph/0611148">Hard Interactions of Quarks and Gluons: A Primer for LHC Physics</a>".

[2] G. Altarelli, G. Parisi. "<a href="https://doi.org/10.1016/0550-3213(77)90384-4">Asymptotic freedom in parton language</a>". Nucl. Phys. B, 126 (1977).

[3] J. Gao,  L. Harland-Lang, J. Rojo. "<a href="http://arxiv.org/abs/arXiv:1709.04922">The Structure of the Proton in the LHC Precision Era</a>".

[4] S. Forte, L. Garrido, J.I. Latorre, A. Piccione. "<a href="http://arxiv.org/abs/hep-ph/0204232">Neural network parametrization of deep inelastic structure functions</a>".

[5] S. Forte. "<a href="http://arxiv.org/abs/arXiv:1011.5247">Parton distributions at the dawn of the LHC</a>".

[6] NNPDF Collaboration. "<a href="http://arxiv.org/abs/arXiv:0808.1231">A Determination of parton distributions with faithful uncertainty estimation</a>".

## Contatti utili

<a href="http://pcteserver.mi.infn.it/~forte/">Prof. Stefano Forte</a>

<a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><img alt="Licenza Creative Commons" style="border-width:0; WIDTH:150px; HEIGHT:20px" src="https://i.creativecommons.org/l/by-nc-nd/4.0/80x15.png" align="middle" /></a><br /><span xmlns:dct="http://purl.org/dc/terms/" property="dct:title">"Il protone intelligente"</span> di<span xmlns:cc="http://creativecommons.org/ns#" property="cc:attributionName"> Giovanni Stagnitto, Associazione Italiana Studenti di Fisica,</span> è distribuito con Licenza <a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/4.0/">Creative Commons Attribuzione - Non commerciale - Non opere derivate 4.0 Internazionale</a>.














